{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe0500",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from PyQt5.QtWidgets import (QApplication, QMainWindow, QPushButton, QVBoxLayout, QHBoxLayout, \n",
    "                             QWidget, QFileDialog, QLabel, QListWidget, QListWidgetItem, QTextEdit, \n",
    "                             QSlider, QScrollArea, QMessageBox, QColorDialog, QLineEdit)\n",
    "from PyQt5.QtGui import QPixmap, QImage, QFont\n",
    "from PyQt5.QtCore import pyqtSignal, Qt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "  \n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class ColorAndClassificationApp(QMainWindow):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Cell Detection Application\")\n",
    "        self.setGeometry(100, 100, 2000, 800)\n",
    "        self.model = model  # image classification model\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # font size(adjust specific layout)\n",
    "        font = QFont()\n",
    "        font.setPointSize(20)\n",
    "\n",
    "        # Main layout\n",
    "        self.main_layout = QHBoxLayout()\n",
    "\n",
    "        # Left Panel\n",
    "        self.left_layout = QVBoxLayout()\n",
    "        self.open_dir_button = QPushButton(\"Open Folder\")\n",
    "        self.open_dir_button.clicked.connect(self.open_folder)\n",
    "        self.image_list = QListWidget()\n",
    "        self.detection_button = QPushButton(\"Detection\")\n",
    "        self.detection_button.clicked.connect(self.perform_detection)\n",
    "        self.detection_all_button = QPushButton(\"Detection all images\")\n",
    "        self.detection_all_button.clicked.connect(self.perform_detection_all_images)\n",
    "        self.left_layout.addWidget(self.open_dir_button)\n",
    "        self.left_layout.addWidget(self.image_list)\n",
    "        self.left_layout.addWidget(self.detection_button)\n",
    "        self.left_layout.addWidget(self.detection_all_button)\n",
    "        \n",
    "        self.open_dir_button.setMinimumSize(150, 80)\n",
    "        self.detection_button.setMinimumSize(150, 80)\n",
    "        self.detection_all_button.setMinimumSize(150, 80)\n",
    "        \n",
    "        # Center Panel\n",
    "        self.center_layout = QVBoxLayout()\n",
    "        self.scroll_area = QScrollArea()\n",
    "        self.image_label = QLabel()\n",
    "        self.image_label.setAlignment(Qt.AlignCenter)\n",
    "        self.scroll_area.setWidget(self.image_label)\n",
    "        self.scroll_area.setWidgetResizable(True)\n",
    "        self.zoom_slider = QSlider(Qt.Horizontal)\n",
    "        self.zoom_slider.setMinimum(50)\n",
    "        self.zoom_slider.setMaximum(100)\n",
    "        self.zoom_slider.setValue(50)\n",
    "        self.zoom_slider.valueChanged.connect(self.zoom_image)\n",
    "        self.center_layout.addWidget(self.scroll_area)\n",
    "        self.center_layout.addWidget(self.zoom_slider)\n",
    "\n",
    "        # Right Panel\n",
    "        self.right_layout = QVBoxLayout()\n",
    "\n",
    "        # Detection 결과를 위한 QTextEdit와 라벨\n",
    "        self.detection_results_area = QTextEdit()\n",
    "        self.detection_results_area.setReadOnly(True)\n",
    "        self.right_layout.addWidget(QLabel(\"Detection Results:\"))\n",
    "        self.right_layout.addWidget(self.detection_results_area)\n",
    "\n",
    "        # Add panels to main layout\n",
    "        self.main_layout.addLayout(self.left_layout, 1)\n",
    "        self.main_layout.addLayout(self.center_layout, 3)\n",
    "        self.main_layout.addLayout(self.right_layout, 1) \n",
    "\n",
    "        # font size adjust\n",
    "        self.image_list.setFont(font)\n",
    "        self.detection_results_area.setFont(font)\n",
    "\n",
    "        central_widget = QWidget()\n",
    "        central_widget.setLayout(self.main_layout)\n",
    "        self.setCentralWidget(central_widget)\n",
    "\n",
    "        # Detection parameters input fields\n",
    "        self.lower_bound_input = QLineEdit(\"70,200,100\")\n",
    "        self.upper_bound_input = QLineEdit(\"140,255,255\")\n",
    "        self.contour_area_input = QLineEdit(\"100\")\n",
    "        \n",
    "        self.image_list.itemClicked.connect(self.display_image)\n",
    "        \n",
    "       #pre-setting before detection\n",
    "        self.processed_pixmap = None\n",
    "        \n",
    "        self.last_mouse_position = None\n",
    "        \n",
    "        self.zoom_percentage_label = QLabel(\"50%\")  # Add a label to display zoom level\n",
    "        self.center_layout.addWidget(self.zoom_percentage_label)  # Add the label to the layout\n",
    "        \n",
    "        self.selected_object_info = None\n",
    "        self.detected_objects = []\n",
    "        self.detected_pixmap = {}\n",
    "        self.is_original_image_displayed = True\n",
    "\n",
    "    def open_folder(self):\n",
    "        file_names, _ = QFileDialog.getOpenFileNames(\n",
    "            self, \"Select Images\", \"\", \"Image Files (*.png *.jpg *.jpeg *.bmp *.gif)\"\n",
    "        )\n",
    "        if file_names:\n",
    "            self.image_list.clear()\n",
    "            for file_name in file_names:\n",
    "                item = QListWidgetItem(os.path.basename(file_name))\n",
    "                item.setData(Qt.UserRole, file_name)\n",
    "                self.image_list.addItem(item)\n",
    "                \n",
    "    def wheelEvent(self, event):\n",
    "        if QApplication.keyboardModifiers() == Qt.ControlModifier:\n",
    "            delta = event.angleDelta()\n",
    "            if delta.y() > 0:\n",
    "                self.zoom_in()\n",
    "            else:\n",
    "                self.zoom_out()\n",
    "            event.accept()\n",
    "        else:\n",
    "            event.ignore()\n",
    "\n",
    "    def zoom_in(self):\n",
    "        current_zoom = self.zoom_slider.value()\n",
    "        new_zoom = min(current_zoom + 10, self.zoom_slider.maximum())\n",
    "        self.zoom_slider.setValue(new_zoom)\n",
    "\n",
    "    def zoom_out(self):\n",
    "        current_zoom = self.zoom_slider.value()\n",
    "        new_zoom = max(current_zoom - 10, self.zoom_slider.minimum())\n",
    "        self.zoom_slider.setValue(new_zoom)\n",
    "\n",
    "    def zoom_image(self, value):\n",
    "        if self.processed_pixmap:\n",
    "            pixmap_to_zoom = self.processed_pixmap\n",
    "        else:\n",
    "            pixmap_to_zoom = self.original_pixmap\n",
    "\n",
    "        if pixmap_to_zoom:\n",
    "            new_width = pixmap_to_zoom.width() * value / 100\n",
    "            new_height = pixmap_to_zoom.height() * value / 100\n",
    "            resized_pixmap = pixmap_to_zoom.scaled(new_width, new_height, Qt.KeepAspectRatio)\n",
    "            self.image_label.setPixmap(resized_pixmap)\n",
    "            self.image_label.adjustSize()\n",
    "            zoom_percentage = (value / self.zoom_slider.maximum()) * 100\n",
    "            self.zoom_percentage_label.setText(f\"{zoom_percentage:.0f}%\")\n",
    "            \n",
    "    \n",
    "    def clear_detection_results(self):\n",
    "        # Reset the image to its original without overlays\n",
    "        if self.original_pixmap:\n",
    "            self.image_label.setPixmap(self.original_pixmap.scaled(self.image_label.width(), self.image_label.height(), Qt.KeepAspectRatio))\n",
    "\n",
    "        # Clear the list of detected objects or any related display components\n",
    "        self.detected_objects = []\n",
    "        self.results_area.clear()  # Assuming this is where detection results are displayed                \n",
    "            \n",
    "    def display_image(self, item):\n",
    "        image_path = item.data(Qt.UserRole)\n",
    "        filename = os.path.basename(image_path)\n",
    "        \n",
    "        if filename in self.detected_pixmap:\n",
    "            self.original_pixmap = self.detected_pixmap[filename]\n",
    "            pixmap = self.original_pixmap.scaled(self.image_label.width(), self.image_label.height(), Qt.KeepAspectRatio)            \n",
    "        else:\n",
    "            self.original_pixmap = QPixmap(image_path)\n",
    "            pixmap = self.original_pixmap.scaled(self.image_label.width(), self.image_label.height(), Qt.KeepAspectRatio)\n",
    "\n",
    "        self.image_label.setPixmap(pixmap)\n",
    "\n",
    "        # reset image information\n",
    "        self.selected_object_info = None\n",
    "        \n",
    "    def perform_detection_all_images(self):\n",
    "        for index in range(self.image_list.count()):\n",
    "            item = self.image_list.item(index)\n",
    "            image_path = item.data(Qt.UserRole)\n",
    "            filename = os.path.basename(image_path)\n",
    "            detected_objects, original_image = self.count_objects_by_color(image_path)\n",
    "\n",
    "            if original_image is None:\n",
    "                continue\n",
    "\n",
    "            # Create a copy of the original image to draw rectangles\n",
    "            image_with_rectangles = original_image.copy()\n",
    "            detection_data = {'Total Cells': 0, 'In Cells': 0, 'Out Cells': 0, 'Fusion Index': 0.0}\n",
    "            detection_data['Total Cells'] = len(detected_objects)\n",
    "            in_count, out_count = 0, 0\n",
    "\n",
    "            for (x, y, w, h) in detected_objects:\n",
    "                object_img = original_image[y:y+h, x:x+w]\n",
    "                object_img = cv2.cvtColor(object_img, cv2.COLOR_BGR2RGB)\n",
    "                object_img_pil = Image.fromarray(object_img)\n",
    "                object_img_tensor = self.transform(object_img_pil).unsqueeze(0).to(self.device)\n",
    "\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    output = self.model(object_img_tensor)\n",
    "                    predicted_prob = torch.sigmoid(output).item()\n",
    "\n",
    "                predicted_class = 0 if predicted_prob >= 0.5 else 1\n",
    "                rect_color = (255, 0, 0) if predicted_class == 1 else (0, 0, 255)\n",
    "\n",
    "                # Draw rectangles on the copy of the image\n",
    "                cv2.rectangle(image_with_rectangles, (x, y), (x + w, y + h), rect_color, 2)\n",
    "\n",
    "                if predicted_class == 1:\n",
    "                    in_count += 1\n",
    "                else:\n",
    "                    out_count += 1\n",
    "                    \n",
    "                self.detected_objects.append((x, y, w, h, predicted_class))\n",
    "            \n",
    "            detection_data['In Cells'] = in_count\n",
    "            detection_data['Out Cells'] = out_count\n",
    "            detection_data['Fusion Index'] = in_count / detection_data['Total Cells'] if detection_data['Total Cells'] > 0 else 0\n",
    "\n",
    "            # Convert the modified image for display\n",
    "            image_with_rectangles = cv2.cvtColor(image_with_rectangles, cv2.COLOR_BGR2RGB)\n",
    "            q_image = QImage(image_with_rectangles.data, image_with_rectangles.shape[1], image_with_rectangles.shape[0], QImage.Format_RGB888)\n",
    "            pixmap = QPixmap.fromImage(q_image)\n",
    "            \n",
    "            # Update the detected_pixmap dictionary\n",
    "            self.processed_pixmap = pixmap\n",
    "            self.detected_pixmap[filename] = pixmap\n",
    "            self.image_label.setPixmap(pixmap.scaled(self.image_label.width(), self.image_label.height(), Qt.KeepAspectRatio))\n",
    "            self.detection_results_area.append(f\"Detected objects: {len(detected_objects)}\")\n",
    "            self.detection_results_area.append(f\"Detected 'IN': {in_count}\\nDetected 'OUT': {out_count}\\n\")\n",
    "            self.detection_results_area.append(f\"Fusion Index (IN/Total) : {in_count/len(detected_objects)}\")\n",
    "            self.export_to_csv(detection_data)\n",
    "\n",
    "        QMessageBox.information(self, \"Detection Completed\", \"Detection has been completed for all selected images.\")\n",
    "\n",
    "\n",
    "    def perform_detection(self):\n",
    "        lower_blue = np.array([70, 200, 100])\n",
    "        upper_blue = np.array([140, 255, 255])\n",
    "        \n",
    "        current_item = self.image_list.currentItem()\n",
    "        if current_item is None:\n",
    "            QMessageBox.information(self, \"Info\", \"No image selected.\")\n",
    "            return\n",
    "        \n",
    "        image_path = current_item.data(Qt.UserRole)\n",
    "        filename = os.path.basename(image_path)\n",
    "        detected_objects, original_image = self.count_objects_by_color(image_path)\n",
    "\n",
    "        if original_image is None:\n",
    "            return\n",
    "\n",
    "        # Create a copy of the original image to draw rectangles\n",
    "        image_with_rectangles = original_image.copy()\n",
    "        \n",
    "        detection_data = {\n",
    "            'Image Name': filename,\n",
    "            'Total Cells': 0,\n",
    "            'In Cells': 0,\n",
    "            'Out Cells': 0,\n",
    "            'Fusion Index': 0.0\n",
    "            }\n",
    "\n",
    "#         detection_data['Total Cells'] = len(detected_objects)\n",
    "        in_count, out_count = 0, 0\n",
    "        for i, (x, y, w, h) in enumerate(detected_objects):\n",
    "            object_img = original_image[y:y+h, x:x+w]\n",
    "            hsv_cropped_img = cv2.cvtColor(object_img, cv2.COLOR_BGR2HSV)\n",
    "            object_img = cv2.cvtColor(object_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            mask = cv2.inRange(hsv_cropped_img, lower_blue, upper_blue)\n",
    "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            object_img_pil = Image.fromarray(object_img)\n",
    "            object_img_tensor = self.transform(object_img_pil).unsqueeze(0).to(self.device)       \n",
    "            \n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = self.model(object_img_tensor.to(device))\n",
    "                predicted_prob = torch.sigmoid(output).item()\n",
    "\n",
    "            predicted_class = 0 if predicted_prob >= 0.5 else 1\n",
    "            rect_color = (255, 0, 0) if predicted_class == 1 else (0, 0, 255)\n",
    "\n",
    "            # Draw rectangles on the copy of the image\n",
    "            cv2.rectangle(image_with_rectangles, (x, y), (x + w, y + h), rect_color, 2)\n",
    "            total_area = sum(cv2.contourArea(contour) for contour in contours if cv2.contourArea(contour) > 100)\n",
    "            \n",
    "            nuclei_count = math.ceil(total_area / 500)  # Estimate the count of nuclei based on average area            \n",
    "\n",
    "            \n",
    "            if predicted_class == 1:\n",
    "                in_count += nuclei_count\n",
    "            else:\n",
    "                out_count += nuclei_count\n",
    "                \n",
    "            self.detected_objects.append((x, y, w, h, predicted_class))\n",
    "    \n",
    "        detection_data['In Cells'] = in_count\n",
    "        detection_data['Out Cells'] = out_count\n",
    "        detection_data['Total Cells'] = in_count + out_count\n",
    "        detection_data['Fusion Index'] = in_count / (in_count + out_count) if len(detected_objects) > 0 else 0\n",
    "\n",
    "        # Convert the modified image for display\n",
    "        image_with_rectangles = cv2.cvtColor(image_with_rectangles, cv2.COLOR_BGR2RGB)\n",
    "        height, width, channel = image_with_rectangles.shape\n",
    "        bytes_per_line = 3 * width\n",
    "        q_image = QImage(image_with_rectangles.data, width, height, bytes_per_line, QImage.Format_RGB888)\n",
    "        pixmap = QPixmap.fromImage(q_image)\n",
    "\n",
    "        # Update the processed pixmap\n",
    "        self.processed_pixmap = pixmap\n",
    "        self.detected_pixmap[filename] = pixmap\n",
    "\n",
    "        self.image_label.setPixmap(pixmap.scaled(self.image_label.width(), self.image_label.height(), Qt.KeepAspectRatio))\n",
    "        self.detection_results_area.append(f\"Detected objects: {in_count + out_count}\")\n",
    "        self.detection_results_area.append(f\"Detected 'IN': {in_count}\\nDetected 'OUT': {out_count}\\n\")\n",
    "        self.detection_results_area.append(f\"Fusion Index(IN/Total) :{in_count/len(detected_objects)}\\n\")\n",
    "        self.export_to_csv(detection_data)\n",
    "\n",
    "    def count_objects_by_color(self, image_path):\n",
    "        # Convert input text to numpy arrays\n",
    "        lower_bound_values = [int(v) for v in self.lower_bound_input.text().split(',')]\n",
    "        upper_bound_values = [int(v) for v in self.upper_bound_input.text().split(',')]\n",
    "        contour_area_threshold = int(self.contour_area_input.text())\n",
    "\n",
    "        # Here we define 'lower_bound' and 'upper_bound' within the method's scope\n",
    "        lower_bound = np.array(lower_bound_values, dtype=\"uint8\")\n",
    "        upper_bound = np.array(upper_bound_values, dtype=\"uint8\")\n",
    "\n",
    "        # Load the image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Error loading image: {image_path}\")\n",
    "            return [], None\n",
    "\n",
    "        # Convert to HSV and apply mask\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        mask = cv2.inRange(hsv, lower_bound, upper_bound)  # Use the locally defined variables\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        detected_objects = []\n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area > contour_area_threshold:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                detected_objects.append((x, y, w, h))\n",
    "\n",
    "        return detected_objects, image\n",
    "\n",
    "#     def export_to_csv(self, data):\n",
    "#         filename = 'detection_results.csv'\n",
    "#         file_exists = os.path.isfile(filename)\n",
    "\n",
    "#         with open(filename, 'a', newline='') as csvfile:\n",
    "#             fieldnames = ['Image Name', 'Total Cells', 'In Cells', 'Out Cells', 'Fusion Index']\n",
    "#             writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "#             if not file_exists:\n",
    "#                 writer.writeheader()  # Write header only once\n",
    "\n",
    "#             writer.writerow(data)\n",
    "\n",
    "    def export_to_csv(self, data):\n",
    "        filename = 'detection_results.csv'\n",
    "        file_exists = os.path.isfile(filename)\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        data['Time'] = timestamp  # Add timestamp to the data dictionary\n",
    "\n",
    "        with open(filename, 'a', newline='') as csvfile:\n",
    "            fieldnames = ['Time', 'Image Name', 'Total Cells', 'In Cells', 'Out Cells', 'Fusion Index']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            if not file_exists:\n",
    "                writer.writeheader()  # Write header only once\n",
    "\n",
    "            writer.writerow(data)\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = CustomCNN().to(device)\n",
    "    model.load_state_dict(torch.load('./model.pth', map_location= device))\n",
    "    app = QApplication(sys.argv)\n",
    "    font = QFont(\"Times New Roman\", 12)\n",
    "    font.setPointSize(20)\n",
    "    #font.setWeight(QFont.Bold)\n",
    "    app.setFont(font)\n",
    "    main_window = ColorAndClassificationApp(model)\n",
    "    main_window.show()\n",
    "    sys.exit(app.exec_())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
